# Model-Optimizer

## Model Inference Performance Optimization

We support customers' online/offline services to achieve model objectives, including reducing latency, improving throughput, and optimizing system resource utilization. The models we optimize include:

### DL Models

- ResNet  
- SwinTransformer  
- ViT  
- BERT  
- Siglip  
- CLIP  

### LLM Models  

For LLM models, based on application scenarios and baseline conditions, we achieve customer-targeted goals through model adaptation, diverse deployment strategies, and performance optimization. By leveraging the latest research advancements, we enable efficient inference, reducing customer costs by 50% to 95%.  

- Vicuna 13B  
- Llama 3.1  
- Qwen 72B  
- MiniCPMV 2.6 
- Llava: Optimize inference performance for models such as Llava-OneVision and Llava-Interleave, achieve over 2x offline throughput improvement based on LMDeploy/TRT-LLM

### Custom VLM

- Support inference optimization and distributed deployment for proprietary models  
- Support NVIDIA GPUs  
- Adapt and optimize for domestic ecosystems such as Huawei Ascend GPUs  

## Project Collaboration  

For model inference optimization, contact **deepindeed2022@gmail.com**. It would be even better if your email includes the following details:  

- Model URL or structural description  
- Target hardware  
- Objective